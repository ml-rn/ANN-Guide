<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>AE Series - rview</title>

    <meta name="author" content="Christoph Martens">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="/design/reveal.css">
    <link rel="stylesheet" href="/design/code-zenburn.css">
    <link rel="stylesheet" href="/design/theme-sky.css">

</head>
<body>

<div class="reveal">
<div class="slides">

	<section>
		<img src="/asset/00-logo.png" width="192" height="192">
		<h2>rview</h2>
		<p>
			<a href="http://github.com/INT-WAW">github.com/INT-WAW</a>
		</p>
	</section>

	<section>
		<section>
			<h1>Overview</h1>
		</section>
		<section>
			<h2>Overview</h2>
			<ul>
				<li>What is Machine Learning?</li>
				<li>What is Deep Learning?</li>
				<li>Different Architectures of NNs</li>
				<li>Why ANNs are so awesome</li>
			</ul>
		</section>
		<section>
			<h2>What is Machine Learning</h2>
			<ul>
				<li>hard to describe, some kind of Buzzword</li>
				<li>Teaching a Machine to do things itself</li>
				<li>Teaching a Machine to describe problems itself</li>
				<li>Teaching a Machine to solve problems itself</li>
			</ul>
		</section>
		<section>
			<h2>Architectures</h2>
			<ul>
				<li>Math sector: NN, ENN, RNN, CNN, Q-RNN</li>
				<li>Biology sector: HTM engine (not covered here)</li>
				<li>Engineering sector: ANN, NEAT, HyperNEAT, ES/HyperNEAT</li>
			</ul>
		</section>
		<section>
			<h2>What does Machine Learning do?</h2>
			<ul>
				<li>Problem -> Machine Learning -> Solution</li>
				<li>Dataset -> Classification -> Yes/No to a question</li>
				<li>Dataset -> Analysis -> Yes/No to a question</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>Neural Networks</h1>
		</section>
		<section>
			<h2>Neural Networks</h2>
			<ul>
				<li>Idea to build a human brain</li>
				<li>can figure things out on their own (in a limited way)</li>
				<li>reward ("good dog!")</li>
				<li>punish ("bad dog!")</li>
			</ul>
		</section>
		<section>
			<h2>Neural Networks</h2>
			<ul>
				<li>How does a Neural Network look like?</li>
			</ul>
			<p>
				<img src="/asset/neural-network.png" alt="neural-network">
			</p>
		</section>
		<section>
			<h2>Backpropagation</h2>
			<ul>
				<li>When being rewarded, NNs will get better over time.</li>
				<li>Backpropagation is just a nested loop. If function is > 0.5, add value to outer Array</li>
			</ul>
		</section>
		<section>
			<h2>Neural Networks</h2>
			<ul>
				<li>can only work with Vector input (0.0 - 1.0)</li>
				<li>are totally dumb, but don't question treatment or punishment</li>
			</ul>
		</section>
		<section>
			<h2>Example Neural Network Usage</h2>
			<p>
				<img src="/asset/neural-network-usage.png" alt="neural-network-usage">
			</p>
		</section>
		<section>
			<h2>Problems with Neural Networks</h2>
			<ul>
				<li>used as so-called Classifier</li>
				<li>Dataset for cars has 4 vectors (4 wheels) as inputs</li>
				<li><a href="">0,1,1,1</a> or whatever...</li>
				<li>NN gets treatment if found broken car (assuming broken occurs less than working cars)</li>
			</ul>
		</section>
		<section>
			<h2>Neural Networks</h2>
			<ul>
				<li>can be seen as dumb Monkeys wanting a banana</li>
				<li>don't question their reward</li>
				<li>don't have a sense of time</li>
				<li>only know good/bad, no matter how long last time was ago</li>
				<li>overfitting problem</li>
				<li>first 1000 cars are good means every car is good. Want banana now.</li>
			</ul>
		</section>
		<section>
			<h2>Problems with Classification</h2>
			<p>
				<img src="/asset/classification-problem.png" alt="classification-problem">
			</p>
		</section>
		<section>
			<h2>Problems with Classification</h2>
			<ul>
				<li>Dataset was not finite anymore (6 wheels more than 4 wheels)</li>
				<li>Let's assume 5000 (insert big number here) 4 wheel cars</li>
				<li>Let's assume 10 6 wheel cars</li>
				<li><a href="">1,1,1,1,1,1</a>  is a good car</li>
				<li><a href="">1,1,1,1,0,0</a>  is a bad car (last 2 vectors were not tracked)</li>
			</ul>
		</section>
		<section>
			<h2>Problems with Infinite/Changing Datasets</h2>
			<ul>
				<li>How to adapt to classifications?</li>
				<li>How to adapt to new dataset vectors?</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>Evolution Cycles</h1>
		</section>
		<section>
			<h2>Evolution Cycles</h2>
			<ul>
				<li>Evolution as Dominance classification</li>
				<li>"Agents" that compete against each other</li>
				<li>Best agents get to mate and produce babies</li>
				<li>Genome (DNA) represents weights of neurons</li>
			</ul>
		</section>
		<section>
			<h2>Evolution Cycles</h2>
			<ul>
				<li>Genome DNA split is randomized</li>
				<li>2 babies are produced, a son and a daughter</li>
				<li>2 babies have more of mothers or fathers genes</li>
			</ul>
			<p>
				<img src="/asset/genome-nn.png" alt="genome-nn">
			</p>
		</section>
		<section>
			<h1>Evolution Cycles</h1>
		</section>
			<ul>
				<li>Evolution always wins</li>
				<li>Randomization always wins</li>
			</ul>
		<section>
			<h2>Problems with Evolution Cycles</h2>
			<ul>
				<li>Adaptive to "bad seasons" (depressive behaviour)</li>
				<li>Adaptive to "good seasons" (optimistic behaviour)</li>
				<li>Needs long time to get better</li>
				<li>Needs parallelization of hundreds of agents</li>
			</ul>
		</section>
		<section>
			<h2>Good things with Evolution Cycles</h2>
			<ul>
				<li>Eventually will always come up with correct classifier</li>
				<li>Adaptive to infinite datasets</li>
				<li>Adaptive to time-based datasets</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>Backpropagation</h1>
		</section>
		<section>
			<h2>Backpropagation</h2>
			<ul>
				<li>The treatment / reward function is typically sigmoid or gaussian</li>
			</ul>
			<p>
				<img src="/asset/sigmoid-gaussian.png" alt="sigmoid-gaussian">
			</p>
		</section>
		<section>
			<h2>Backpropagation</h2>
			<ul>
				<li>If function result is bigger than 0.5, increase neuron value</li>
				<li>If function result is lower than 0.5, decrease neuron value</li>
			</ul>
		</section>
		<section>
			<h2>Problems with Backpropagation</h2>
			<p>
				<img src="/asset/problems-backpropagation.png" alt="problems-backpropagation">
			</p>
		</section>
	</section>
	<section>
		<section>
			<h1>Recurrent Neural Networks</h1>
		</section>
		<section>
			<h2>Recurrent Neural Networks</h2>
			<ul>
				<li>so-called unfolding neural networks</li>
				<li>hidden layers will unfold in another dimension</li>
				<li>unfolding leads to sense of time/occurance</li>
			</ul>
		</section>
		<section>
			<h2>Problems with Neural Networks</h2>
			<ul>
				<li>It's super ineffective (computation time)</li>
				<li>Each change in dataset vectors needs manual correction of reward function</li>
				<li>Eventually RNNs will always lead to hacking the reward function</li>
				<li>Mostly RNNs will end up with a single uber strategy</li>
			</ul>
		</section>
		<section>
			<h2>LSTM Recurrent Neural Networks</h2>
			<ul>
				<li>Long Short Term Memory RNNs</li>
				<li>forget data and training that was irrelevant</li>
				<li>do so by introducing forget gates</li>
			</ul>
		</section>
		<section>
			<h2>LSTM RNN</h2>
			<p>
				<img src="/asset/lstm-rnn.png" alt="lstm-rnn">
			</p>
		</section>
		<section>
			<h2>LSTM RNN</h2>
			<ul>
				<li>values close to zero (neuron output) are ignored</li>
				<li>good at identifying positive values</li>
				<li>very bad at identifying negative values</li>
			</ul>
		</section>
		<section>
			<h2>Problems with LSTM RNN</h2>
			<ul>
				<li>very good at reoccuring datasets</li>
				<li>basically the computer variant of captain obvious</li>
				<li>bad for identifying new strategies</li>
				<li>new strategies require combination of multiple RNNs (in line or a graph)</li>
			</ul>
		</section>
		<section>
			<h2>Deep Learning</h2>
			<ul>
				<li>Combination of Neural Networks</li>
				<li>Most companies use RNNs because they do picture or time-based data stuff anyways</li>
				<li>Probability-based assumptions for determining results</li>
				<li>focus on having many tiny (easy to compute) neural networks</li>
				<li>tiny neural networks aim to exist one specified purpose</li>
			</ul>
		</section>
		<section>
			<h2>Deep Learning</h2>
			<p>
				<img src="/asset/deep-learning.png" alt="deep-learning">
			</p>
		</section>
	</section>
	<section>
		<section>
			<h1>Adaptive Neural Networks</h1>
		</section>
		<section>
			<h2>Adaptive Neural Networks</h2>
			<p>
				<img src="/asset/adaptive-nn.png" alt="adaptive-nn">
			</p>
		</section>
		<section>
			<h2>Problems with Adaptive Neural Networks</h2>
			<ul>
				<li>Randomization always wins</li>
				<li>Always better than RNNs without manual influence</li>
				<li>Unsupervised learning effect</li>
				<li>Lower efficiency than Evolution (parallelization) concepts</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>NEAT</h1>
		</section>
		<section>
			<h2>NEAT</h2>
			<ul>
				<li>Neuroevolution of augmenting topologies</li>
				<li>genetic algorithm that generates ANNs</li>
				<li>finds best fitness of evolved solutions</li>
				<li>respects diversity by tracking genes with history markes</li>
				<li>can predict what genes will be dominant in future</li>
				<li>can predict dominant species in future</li>
				<li>tracks history, crossover techniques, speciation</li>
			</ul>
		</section>
		<section>
			<h2>NEAT</h2>
			<ul>
				<li>typical NEAT implementation has agents</li>
				<li>best agents get to make babies based on their fitness</li>
				<li>starts minimally and predicts maximum efficiency in growth</li>
				<li>can predict where to spawn neurons to get more effective</li>
				<li>only problem is unwanted cross-talk between near-end neurons</li>
			</ul>
		</section>
		<section>
			<h2>Problems with NEAT</h2>
			<ul>
				<li>Adaptive to anything</li>
				<li>Figures out anything itself, given enough time</li>
				<li>Mutates connections between neurons (so-called substrates)</li>
				<li>Mutates nodes (neurons) in locations</li>
				<li>history markers are awesome for prediction</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>HyperNEAT</h1>
		</section>
		<section>
			<h2>HyperNEAT</h2>
			<ul>
				<li>Hypercube based encoding for NEAT</li>
				<li>CPPN usage (Compositional Pattern Producing NN)</li>
				<li>Evolutionary algorithms generate neural network</li>
				<li>Describes patterns of connectivity (called encoding)</li>
				<li>Represents patterns like symmetry, repetition and variation of substrates</li>
			</ul>
		</section>
		<section>
			<h2>HyperNEAT</h2>
			<ul>
				<li>4D Hypercube (x1 , x2 , y1 , y2) for substrates</li>
				<li>2D and 3D space is called a substrate</li>
			</ul>
		</section>
		<section>
			<h2>Problems with HyperNEAT</h2>
			<ul>
				<li>Basically a shitload of neurons</li>
				<li>Aweomse prediction of geometric relations</li>
				<li>Can see if sensors (inputs) are related to each other</li>
				<li>does not require adaption of reward function</li>
				<li>does not exploit reward function</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>ES-HyperNEAT</h1>
		</section>
		<section>
			<h2>ES-HyperNEAT</h2>
			<ul>
				<li>Evolvable substrate HyperNEAT</li>
				<li>places connections (substrates) automatically</li>
				<li>respects neuron cloud density</li>
				<li>respects neuron cloud locations</li>
				<li>can increase substrates via evolution</li>
			</ul>
		</section>
		<section>
			<h2>Problems with HyperNEAT</h2>
			<ul>
				<li>exponential growth of neurons via time</li>
				<li>geometry decides where to place neurons</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>RTES-HyperNEAT</h1>
		</section>
		<section>
			<h2>RTES-HyperNEAT</h2>
			<ul>
				<li>Multi-agent implementation of ES-HyperNEAT</li>
				<li>made for realtime (RT) usage</li>
				<li>uses limited epoches for dominance of agents</li>
				<li>best agents produce babies based on fitness</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>lychee.js CARTEL</h1>
		</section>
		<section>
			<h2>lychee.js CARTEL</h2>
			<ul>
				<li>botnet size (read: computers in the cloud) is above 500k+</li>
				<li>each bot runs hundreds / thousands of agents in parallel</li>
				<li>ES/HyperNEAT is really slow</li>
				<li>RTES/HyperNEAT is pretty good, but still very static in inputs</li>
				<li>Humans have no clue which inputs are relevant</li>
				<li>Humans have no clue which training data is relevant</li>
			</ul>
		</section>
		<section>
			<h2>CARTEL/ES-HyperNEAT</h2>
			<ul>
				<li>you can basically forget all RNN solutions for code-analysis</li>
				<li>ES-HyperNEAT is the closest to a problem where you have no clue</li>
				<li>adaptive ANN required that learns which inputs play a role</li>
				<li>adaptive ANN required that learns which training data plays a role</li>
				<li>computation time in a botnet (peer-cloud) is very limited with ES/HyperNEAT</li>
				<li>focus of the CARTEL is on performance, cache optimization and delegation</li>
			</ul>
		</section>
		<section>
			<h2>CARTEL/ES-HyperNEAT</h2>
			<ul>
				<li>Clone-Adaptive Real-Time Evolvable Legation / Evolvable-Substrate HyperNEAT</li>
				<li>adaptive to varying inputs</li>
				<li>adaptive to legation (voted "minister" on each bot for botnet synchronization)</li>
				<li>hashing of inputs/outputs with murmur hash (ultrafast non-crypto hashing)</li>
				<li>delegating agent clones to do the same thing behind the scenes</li>
				<li>proxying clones to increase fitness based on occurance</li>
			</ul>
		</section>
		<section>
			<h2>CARTEL/ES-HyperNEAT</h2>
			<ul>
				<li>decides when randomization is good for evolution</li>
				<li>agent clones can be targeted with opposite datasets</li>
				<li>better variance for sigmoid behaviour</li>
				<li>better variance for gaussian behaviour</li>
				<li>sigmoid vs. gaussian still have to be evaluated (currently being evaluated)</li>
			</ul>
		</section>
		<section>
			<h2>CARTEL/ES-HyperNEAT</h2>
			<ul>
				<li>Academic work still needs to be done</li>
				<li>Reference implementation public around September 2016</li>
				<li>needs some polishing work and well, much code decoupling to be not lychee.js centric</li>
				<li>Anybody want to write some papers and help?</li>
			</ul>
		</section>
	</section>
	<section>
		<section>
			<h1>Summary</h1>
		</section>
		<section>
			<h2>Summary</h2>
			<ul>
				<li>github.com/cookiengineer/ANN-Guide (this talk)</li>
				<li>github.com/Artificial-Engineering/lycheejs (reference implementation)</li>
				<li>github.com/Artificial-Engineering/lycheejs-cartel (zero content by now)</li>
			</ul>
		</section>
	</section>


</div>
</div>

<script src="/source/head.min.js"></script>
<script src="/source/reveal.js"></script>
<script src="/source/preview.js"></script>

</body>
</html>
